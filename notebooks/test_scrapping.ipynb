{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    txt = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    txt = txt.replace('  ', ' ')\n",
    "    return txt.strip()\n",
    "\n",
    "def extract_by_regex(text: str, regex: str) -> str:\n",
    "    pattern = re.compile(regex)\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        # Check if there are any groups and return the first group if it exists\n",
    "        if match.groups():\n",
    "            return match.group(1) + \" \" + match.group(2) if len(match.groups()) > 1 else match.group(1)\n",
    "        else:\n",
    "            return match.group(0)  # Return the entire match if no groups are defined\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripAdvisorScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = None\n",
    "\n",
    "\n",
    "    def fetch_page(self):\n",
    "        \n",
    "        # Set headers\n",
    "        random_request_id = \"\".join(\n",
    "                random.choice(string.ascii_lowercase + string.digits) for i in range(180)\n",
    "            )\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            'accept-language': 'en-US,en;q=0.9,fr;q=0.8',\n",
    "            \"X-Requested-By\": random_request_id,\n",
    "            \"Referer\": \"https://www.tripadvisor.com/Hotels\",\n",
    "            \"Origin\": \"https://www.tripadvisor.com\",\n",
    "            'accept-encoding': 'gzip, deflate, br',\n",
    "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'encoding': 'utf-8',\n",
    "        }\n",
    "        \n",
    "        # Send a GET request\n",
    "        response = requests.get(self.url, headers=headers)\n",
    "        try:\n",
    "            # Check encoding\n",
    "            \n",
    "            if response.headers.get('Content-Encoding') == 'gzip':\n",
    "                content = response.content.decode('gzip')\n",
    "            elif response.headers.get('Content-Encoding') == 'deflate':\n",
    "                content = response.content.decode('zlib')\n",
    "            # elif response.headers.get('Content-Encoding') == 'br':\n",
    "            #     content = brotli.decompress(response.content)\n",
    "            else:\n",
    "                content = response.text \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            content = response.text\n",
    "        finally:    \n",
    "            self.soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    def print_soup(self):\n",
    "        if self.soup:\n",
    "            print(self.soup.prettify())\n",
    "        else:\n",
    "            print(\"Soup is not initialized. Please fetch the page first.\")\n",
    "\n",
    "    def get_review_cards(self):\n",
    "        if self.soup:\n",
    "            review_cards = self.soup.find_all('div', class_='_c', attrs={'data-automation': 'reviewCard'})\n",
    "            return review_cards\n",
    "        else:\n",
    "            print(\"Soup is not initialized. Please fetch the page first.\")\n",
    "            return []\n",
    "        \n",
    "    def get_next_url(self, base_url):\n",
    "        next_url = self.soup.find('a', class_='BrOJk u j z _F wSSLS tIqAi unMkR').get('href')\n",
    "        if next_url is not None:\n",
    "            next_url = base_url + next_url\n",
    "            return next_url\n",
    "        else:\n",
    "            return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cards_to_df(review_cards):\n",
    "    corpus = []\n",
    "    for card in review_cards:\n",
    "        # Extract the review text\n",
    "        review_text = 'biGQs _P pZUbB KxBGd'\n",
    "        contributions = 'biGQs _P pZUbB osNWb'\n",
    "        date = 'biGQs _P pZUbB ncFvv osNWb'\n",
    "        user_name = 'biGQs _P fiohW fOtGX'\n",
    "        rating = 'UctUV d H0'\n",
    "\n",
    "        scrap_review_text = card.find('div', {'class': review_text}).text if card.find('div', {'class': review_text}) else None\n",
    "        scrap_contributions = card.find('div', {'class': contributions}).text if card.find('div', {'class': contributions}) else None\n",
    "        scrap_date = card.find('div', {'class': date}).text if card.find('div', {'class': date}) else None\n",
    "        scrap_user_name = card.find('span', {'class': user_name}).text if card.find('span', {'class': user_name}) else None\n",
    "        scrap_rating = card.find('svg', {'class': rating}).find('title').text if card.find('svg', {'class': rating}) else None\n",
    "\n",
    "        doc = {\n",
    "            'review_text': clean_text(scrap_review_text) if scrap_review_text != None and scrap_review_text != '' else None,\n",
    "            'rating': extract_by_regex(scrap_rating, r'(\\d\\.\\d) of 5 bubbles') if scrap_rating != None and scrap_rating != '' else None,\n",
    "            'user_name': scrap_user_name if scrap_user_name != None and scrap_user_name != '' else None,\n",
    "            'date': extract_by_regex(scrap_date, r'(\\w+ \\d+), (\\d+)') if scrap_date != None and scrap_date != '' else None,\n",
    "            'contributions': extract_by_regex(scrap_contributions, r\"\\d+\") if scrap_contributions is not None and scrap_contributions != '' else None\n",
    "        }\n",
    "        corpus.append(doc)\n",
    "\n",
    "    df = pd.DataFrame(corpus)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url_base = 'https://www.tripadvisor.com'\n",
    "complement = '/Restaurant_Review-g187265-d12419021-Reviews-L_Auberge_Des_Canuts-Lyon_Rhone_Auvergne_Rhone_Alpes.html'\n",
    "url = url_base + complement\n",
    "\n",
    "\n",
    "scraper = TripAdvisorScraper(url)\n",
    "scraper.fetch_page()\n",
    "\n",
    "next_url = scraper.get_next_url(url_base)\n",
    "\n",
    "review_cards = scraper.get_review_cards()   \n",
    "\n",
    "df = cards_to_df(review_cards)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>user_name</th>\n",
       "      <th>date</th>\n",
       "      <th>contributions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lovey simple meal at a bouchon. Good size port...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AngryofTollcross</td>\n",
       "      <td>October 18 2024</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I had a lunch/dinner here ordering off one of ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>graceinbeijing</td>\n",
       "      <td>March 1 2020</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We made the reservation with \"the Fork\" and wa...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Laura M</td>\n",
       "      <td>October 27 2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We could clearly see the hygiene and cleanline...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tara Bee</td>\n",
       "      <td>June 11 2022</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Upon arrival, the real welcome in cork: large ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Luna T</td>\n",
       "      <td>October 28 2024</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I see that others have given this tiny place l...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Mardi S</td>\n",
       "      <td>October 14 2022</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>There’s good attentive service, and there’s be...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tim W</td>\n",
       "      <td>February 5 2023</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>After a (discreet) dinner with work colleagues...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Gianluca P</td>\n",
       "      <td>May 27 2024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I'm not one to write reviews when they're nega...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Óliver R</td>\n",
       "      <td>June 3 2024</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We come to Lyon regularly and we have frequent...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Pascal Perret</td>\n",
       "      <td>March 11 2024</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I had made a reservation with The Fork. It too...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Andres R</td>\n",
       "      <td>August 29 2023</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>This little restaurant is a gem The place is w...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Sabine S</td>\n",
       "      <td>November 10 2023</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Great local dishes enjoyed with good friends a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>John V</td>\n",
       "      <td>October 12 2023</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Excessive price compared to the quantity (as a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Famille badel</td>\n",
       "      <td>April 1 2024</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>very big disappointment with a fillet of Charo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Trek61489695964</td>\n",
       "      <td>August 27 2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          review_text rating  \\\n",
       "0   Lovey simple meal at a bouchon. Good size port...    4.0   \n",
       "1   I had a lunch/dinner here ordering off one of ...    4.0   \n",
       "2   We made the reservation with \"the Fork\" and wa...    5.0   \n",
       "3   We could clearly see the hygiene and cleanline...    1.0   \n",
       "4   Upon arrival, the real welcome in cork: large ...    5.0   \n",
       "5   I see that others have given this tiny place l...    4.0   \n",
       "6   There’s good attentive service, and there’s be...    1.0   \n",
       "7   After a (discreet) dinner with work colleagues...    1.0   \n",
       "8   I'm not one to write reviews when they're nega...    1.0   \n",
       "9   We come to Lyon regularly and we have frequent...    5.0   \n",
       "10  I had made a reservation with The Fork. It too...    1.0   \n",
       "11  This little restaurant is a gem The place is w...    5.0   \n",
       "12  Great local dishes enjoyed with good friends a...    5.0   \n",
       "13  Excessive price compared to the quantity (as a...    1.0   \n",
       "14  very big disappointment with a fillet of Charo...    1.0   \n",
       "\n",
       "           user_name              date contributions  \n",
       "0   AngryofTollcross   October 18 2024            10  \n",
       "1     graceinbeijing      March 1 2020            71  \n",
       "2            Laura M   October 27 2020             1  \n",
       "3           Tara Bee      June 11 2022           395  \n",
       "4             Luna T   October 28 2024            61  \n",
       "5            Mardi S   October 14 2022           156  \n",
       "6              Tim W   February 5 2023             3  \n",
       "7         Gianluca P       May 27 2024             1  \n",
       "8           Óliver R       June 3 2024             4  \n",
       "9      Pascal Perret     March 11 2024            39  \n",
       "10          Andres R    August 29 2023             3  \n",
       "11          Sabine S  November 10 2023            45  \n",
       "12            John V   October 12 2023             6  \n",
       "13     Famille badel      April 1 2024             8  \n",
       "14   Trek61489695964    August 27 2023             1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 2 done\n",
      "Page 3 done\n",
      "Page 4 done\n",
      "Page 5 done\n",
      "Page 6 done\n",
      "Page 7 done\n",
      "Page 8 done\n",
      "Page 9 done\n",
      "Page 10 done\n",
      "Page 11 done\n",
      "Page 12 done\n",
      "Page 13 done\n",
      "Page 14 done\n",
      "Page 15 done\n",
      "Page 16 done\n",
      "Page 17 done\n",
      "Page 18 done\n",
      "Page 19 done\n",
      "Page 20 done\n",
      "Page 21 done\n",
      "Page 22 done\n",
      "Page 23 done\n",
      "Page 24 done\n",
      "Page 25 done\n",
      "Page 26 done\n",
      "Page 27 done\n",
      "Page 28 done\n",
      "Page 29 done\n",
      "Page 30 done\n",
      "Page 31 done\n",
      "Page 32 done\n",
      "Page 33 done\n",
      "Page 34 done\n",
      "Page 35 done\n",
      "Page 36 done\n",
      "Page 37 done\n",
      "Page 38 done\n",
      "Page 39 done\n",
      "Page 40 done\n",
      "Page 41 done\n",
      "Page 42 done\n",
      "Page 43 done\n",
      "Page 44 done\n",
      "Page 45 done\n",
      "Page 46 done\n",
      "Page 47 done\n",
      "Page 48 done\n",
      "Page 49 done\n",
      "Page 50 done\n",
      "Page 51 done\n",
      "Page 52 done\n",
      "Page 53 done\n",
      "Page 54 done\n",
      "Page 55 done\n",
      "Page 56 done\n",
      "Page 57 done\n",
      "Page 58 done\n",
      "Page 59 done\n",
      "Page 60 done\n",
      "Page 61 done\n",
      "Page 62 done\n",
      "Page 63 done\n",
      "Page 64 done\n",
      "Page 65 done\n",
      "Page 66 done\n",
      "Page 67 done\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m review_cards \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mget_review_cards()\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, cards_to_df(review_cards)], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m next_url \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m page \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 59\u001b[0m, in \u001b[0;36mTripAdvisorScraper.get_next_url\u001b[0;34m(self, base_url)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_next_url\u001b[39m(\u001b[38;5;28mself\u001b[39m, base_url):\n\u001b[0;32m---> 59\u001b[0m     next_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBrOJk u j z _F wSSLS tIqAi unMkR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m     next_url \u001b[38;5;241m=\u001b[39m base_url \u001b[38;5;241m+\u001b[39m next_url\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_url\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "page = 1\n",
    "while next_url is not None:\n",
    "    time.sleep(random.uniform(1, 5) / 1000)\n",
    "    scraper = TripAdvisorScraper(next_url)\n",
    "    scraper.fetch_page()\n",
    "    review_cards = scraper.get_review_cards()\n",
    "    df = pd.concat([df, cards_to_df(review_cards)], ignore_index=True)\n",
    "    next_url = scraper.get_next_url(url_base)\n",
    "    page += 1\n",
    "    print(f\"Page {page} done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
